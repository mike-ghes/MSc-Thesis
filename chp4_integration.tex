\chapter{Integration over Hybrid Domains}
\label{chp:Integration}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTRODUCTION
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\todo[inline]{Integral is over hybrid set *of a special form*, not just over any old hybrid set.}


In many ways, integration provides the inspiration for hybrid set domains.
As such, many of the techniques we have been using will be very familiar when placed back within their original context.
That being said, notationally, sets and orientation are treated as often treated as distinct objects rather than a single entity.


As a hopefully illustrative example of this, consider a typical definition of the \textbf{definite integral} from an introductory
course in calculus.
Given a function $f$ with real variable $x$ and an interval $[a,b)$ of the (extended) real line
\footnote{ The extended real line denoted $\extendedreal$ is the set of real numbers as well as the points at 
$+\infty$ and $-\infty$} the definite integral
\begin{equation*}
	\int_a^b f(x) \diff x
\end{equation*}
is defined as the signed area bounded by $f$ between $x=a$ and $x=b$.
Generally after a short exposition about Riemann sums, it would then be revealed that if $F$ is an anti-derivative of the
 function $f$ then:
\begin{equation*}
	\int_a^b f(x) \diff x \;=\; F(b)-F(a) \;=\; - (F(a)-F(b)) \;=\; - \int_b^a f(x) \diff x
\end{equation*}


However this means that previously defining the definite integral using (unoriented) intervals was a bit of a misnomer.
As we saw in the previous chapter, when $a \geq b$, the interval $[a,b) = \{ x \;|\; a \leq x < b \}$ is the empty set.
So the interval itself cannot really be thought of as a part of the definite integral.
If it were the interval itself that we were concerned with then for $a \leq b$, the interval $[b,a)$ is empty,
as are the intervals $[a,a)$, $[\pi, e)$, and $[\infty, -\infty)$.
As these are all different representations of the same interval, and if we were truly concerned with 
the relationship between $f$ and the interval, then one might argue that:
\begin{equation*}
	\int_b^a f(x) \diff x \;=\; \int_a^a f(x) \diff x \;=\; \int_\pi^e f(x)\diff x \;=\; \int_\infty^{-\infty} f(x) \diff x
\end{equation*}
Obviously this is not the intent but there is a distinct mismatch between the conceptual usefulness of considering
integrals as over an interval.
But when actually using said interval, it is treated as an ordered pair of endpoints disregarding the set itself.


This issue is exasperated working with the more general notation
\begin{equation*}
	\int_X f(x) \diff x
\end{equation*}
which denotes integrating $f$ over a set $X$.
Now there is nothing stopping $X$ from an interval and one would very much like to say that we can convert between the
two notations with a definition like:
\begin{equation*}
	\int_a^b f(x) \diff x \;=\; \int_{[a,b)} f(x) \diff x
\end{equation*}
but there is no analogous translation to $\int_a^b = - \int_b^a$ and so if we made this assertion we would be left with
\begin{equation*}
	\int_{[a,b)} f(x) \diff x 
		= \int_a^b f(x) \diff x 
		= -\int_b^a f(x) \diff x 
		= - \int_{[b,a)} f(x)\diff x 
		= -\int_\emptyset f(x)\diff x 	
		= 0
\end{equation*}
This is clearly not a desired outcome; instead what is actually intended is an oriented interval.
The integral should instead be over the hybrid set $[\![a,b)\!)$.



Another advantage to using oriented sets is a more natural language for manipulating domains of integration than sets.
We cannot add sets (in the traditional, non-$\sigma$-algebra sense) 
but with the point-wise sum $\oplus$, we \emph{can} add hybrid sets.
This allows us to say that the integral operator is \emph{bi-linear}.
By this we mean, that integration is a function of two operands, the integrand and the domain.
Integration is linear over integrands regardless,
\begin{equation*}
	\int_{X} f(x) + g(x) \diff x = \int_X f(x) \diff x + \int_X g(x) \diff x
\end{equation*}
but with summation defined on the domain as well
\begin{equation*}
	\int_{X\oplus Y} f(x) \diff x = \int_{X} f(x) \diff x + \int_Y f(x) \diff x
\end{equation*}


In one dimension, many of these changes may seem trivial advances but in higher dimensions, 
the oriented and measure-theoretic approaches diverge \cite{tao2007differential}.
A Riemann integration foundation is not overly concerned with negatively oriented intervals and the definitions continue
to function unperturbed by the fact that the ``right-hand'' bound is actually less than the ``left-hand'' bound.
As such extending to higher dimension Riemann integrals, orientation is easily bundled in as well.
But measure-based approaches to integration need to fake orientation in one dimension.
First one must convert $\int_a^b$ into either $-\int_{[a,b)}$ or $\int_{[a,b)}$ and then integrate one or the other.
Once one commits to an orientation, there is no coming back.
In higher dimensions, this becomes more of a problem.

This is usually forgivable given the extra power the Lebesgue integral affords over the Riemann.
Using hybrid sets as domains of integration allow us to use the best features of both.
In this chapter we will investigate integration using hybrid set domains.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% HYBRID INTEGRATION
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% RIEMANN
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Riemann Integral on $k$-rectangles}

\begin{definition}
	Let $[\![\boldsymbol{a}, \boldsymbol{b}]\!]$ be a $k$-rectangle in $\mathbb{R}^n$ 
	where $\boldsymbol{a}=(a_1,\ldots, a_n)$ and $\boldsymbol{b}=(b_1,\ldots, b_n)$. 
	We denote the \textbf{volume of $\boldsymbol{[\![a,b]\!]}$} with $\text{vol}$ and define it as:
	\begin{equation}
		\vol(\; [\![\boldsymbol{a}, \boldsymbol{b} ]\!] \;) 
			= (b_1 - a_1) \cdot (b_2 - a_2) \cdot \ldots \cdot (b_n - a_n)
	\end{equation}
\end{definition}

We say volume, but this depends on the dimension.
In 2-dimensions, this would better be described as area and in 1-dimension as a length.
For any $k<n$, a $k$-rectangle will have volume zero.
This is fitting, as we wished for example to measure the area of rectangle in 2 dimensions, but the same 2-rectangle in
$\mathbb{R}^3$ is flat and can hold no volume.
In at least one dimension, the cube will be degenerate (i.e. $a_i = b_i$) and so will contribute zero to the product.
Additionally, one can also observe that $\text{vol}( \ominus [\![\boldsymbol{a}, \boldsymbol{b} ]\!]) 
= - \text{vol}( [\![ \boldsymbol{a}, \boldsymbol{b} ]\!] )$.


The next task is to partiton the $k$-rectangle $[\![\boldsymbol{a}, \boldsymbol{b}]\!]$ into a grid of smaller
$k$-rectangles.
To do this, for each dimension $[\![a_i, b_i]\!]$, we choose a generalized partition $P_i$ such that $P_i$ is composed of
oriented intervals.
To build our mesh, we construct smaller $k$-rectangles $I_{i_1, \ldots, i_n}$ using the Cartesian product of pieces:
\begin{equation*}
	I_{p_1, \ldots, p_n} = p_1 \times \ldots \times p_n
\end{equation*}
where each $p_i$ is taken from $P_i$.
We are now ready to construct Riemann sums.

\begin{definition}
	Given $P=\{ P_j \}_{j=1}^n$ where $P_j$ is an interval generalized partition of $[\![a_j, b_j]\!]$,
	and $f:[\![\boldsymbol{a}, \boldsymbol{b}]\!] \to \mathcal{R}$ then we define a Riemann sum $f_P$ to be:
	\begin{equation}
		f_P = \sum_{p_1 \in P_1} \ldots \sum_{p_n \in P_n} f(x^*_{p_1, \ldots, p_n}) \vol(I_{p_1, \ldots, p_n})
	\end{equation}
	where $x^*_{p_1, \ldots, p_n}$ is any point chosen from $I_{p_1, \ldots, p_n}$.
\end{definition}

Note that we specify \emph{a} Riemann sum, not \emph{the} Riemann sum.
There are several ways to choose $x^*_{i_1,\ldots,i_n} \in I_{i_1, \ldots, i_n}$ and different samplings can lead to different Riemann sums for the same partition and same function.
In $\mathbb{R}^1$, several common ways to sample include the left and right Riemann sums (which sample at the left and right boundaries), the trapezoidal Riemann sum (which averages the left and right) the upper Riemann sum (which samples at $\max(f(x_{i_1, \ldots, i_n}))$) and the lower Riemann sum (which samples at $\min(f(x_{i_1,\ldots,i_n}))$).


\begin{figure}[ht]
\caption[Riemann Integral]{Upper and lower Riemann sums shown for the same sequence shown with light and dark rectangles respectively. A function over an oriented interval is Riemann integrable if the two sums converge.}
\centering
\includegraphics[scale=0.6]{diagrams/riemann}
\end{figure}


\begin{definition}
The Riemann integral of a function $f:\mathbb{R}^n \to \mathbb{R}$ over a $k$-rectangle 
$[\![\boldsymbol{a}, \boldsymbol{b}]\!]$ as $\max( | \vol( I_{p_1,\ldots, p_n}) | : p_i \in P_i )$ goes to zero.
	\begin{equation}
		\int_{[\![a,b]\!]} f(x) \diff x = f_P
	\end{equation} 
	We say that a function is \textbf{Riemann integrable} if the upper and lower Riemann sums converge.
\end{definition}


\todo[inline]{Don't understand $f_P$}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LEBESGUE INTEGRAL
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{The Lebesgue Integral on hybrid domains}

Another common approach to integration is the Lebesgue integral which approaches integration from a measure theoretic
perspective.
We begin our construction with a $\sigma$-algebra (read sigma-algebra) over a set $X$.
By this we mean a collection of subsets of $X$ which are closed under countable complement, union and intersection.
The important part here is that we have a closed universe of sets on which we can perform 
\emph{nearly} arbitrary set operations but remain within said universe.
Next we can attach to a $\sigma$-algebra a measure to form a \textbf{measure space} $(X, \Sigma, \mu)$ 
where $X$ is a set $\Sigma$ is a $\sigma$-algebra  over subsets of $X$ and $\mu$ is a measure defined on the sets in
 $\Sigma$.
This measure $\mu$ is a function $\mu: \Sigma \to \extendedreal$ with the following properties:
\begin{description}
	\item[Non-negative:] For all $E \in \Sigma$, $\mu(E) \geq 0$
	\item[Empty set has measure 0:] $\mu(\emptyset) = 0$
	\item[Countably Additive:] For $\{E_i\}_i$, a countable set of disjoint sets in $\Sigma$,
		$\mu \left( \bigcup_i E_i \right) = \sum_i \mu(E_i)$
\end{description}
Within the context of integration, the Borel and Lebesgue measure spaces are notable constructions.
Both of which give a suitably large universe of sets for which to integrate over.
Certainly much more than the set of Riemann integrable domains.
Finally, given a measure space $(X, \Sigma, \mu)$, we say that $f : X \to \mathbb{R}$ is a \textbf{measurable function}
if  $\{ x \; | \; f(x) > t\}$ is a measurable set for all $t$.



If $\ind[S]$ is indicator function $\ind[S] : X \to \{ 0, 1 \}$ given by 
$\ind[S] (x) = 1$ if $x \in S$ and $\ind[S] (x) = 0$ otherwise.
Clearly if $S$ is a measurable set, then $\ind[S]$ is a measurable function.
We will use this as a base case.
Given a measure space $(X, \Sigma, \mu)$ and $S \in \Sigma$, we define the integral:
\begin{equation*}
	\int \! \ind[S] \diff \mu = \mu ( S )
\end{equation*}


From this, we consider functions which are the sum of indicator functions.
We say that $s$ is a \textbf{simple function} if there are finite sets of measurable sets and $\{ A_k \}_{k=0}^n$
and matching real coefficients $\{ a_k \}_{k=0}^n$ such that:
\begin{equation*}
	s = \sum_{k=0}^n a_k \ind[A_k]
\end{equation*}
The integral of a simple function is then easily defined linearly in terms of integrals of indicator functions:
\begin{equation*}
	\int \! s \diff \mu 
		= \int \left(\sum_{k=0}^n a_k \ind[A_k] \right) \diff \mu 
		= \sum_{k=0}^n a_k  \int \! \ind[A_k] \diff \mu
		= \sum_{k=0}^n a_k \cdot \mu(A_k)
\end{equation*}

But we don't always wish to integrate over the entire measure space but rather some measurable subset of $X$.
This would be done with the notation $\int_B$ instead of $\int$ and replacing $\mu(A_k)$ with $\mu(A_k \cap B)$.
If $A_k$ and $B$ are both measurable sets in $\Sigma$ then their intersection is also a measurable set in $\Sigma$.
But as already mentioned, integrating over sets is a misnomer; we should be integrating over oriented sets.
To do this, we extend $\mu$ linearly for hybrid sets on $\Sigma$.
Strictly speaking, doing so means that $\mu$ is no longer a measure as it takes on negative values.
For a hybrid set $H \in \hsetover[\Sigma]$,
\begin{equation*}
	\int_H s \diff \mu = \int H \cdot s \diff \mu = \sum_k a_k \cdot \mu(A_k \otimes H)
\end{equation*}


We then use simple functions to approximate general measurable functions.
For a non-negative function $f$, we say this is the largest simple function that is everywhere less than $f$:
\begin{equation*}
	\int_H \! f \diff\mu = 
		\text{sup} \left\{ 
			\int_H s \diff \mu \;\middle|\; s \text{ simple, and } 0 \leq \varphi \leq f 
		\right\}
\end{equation*}
But even if $f$ takes negative values, we can split it into positive and negative parts by:
\begin{align*}
f^+(x) &:= \mathrm{max}( 0, f(x)) \\
f^-(x) &:= \mathrm{max}(0, -f(x)) 
\end{align*}
Both $f^+$ and $f^-$ are clearly non-negative but also, observe that $f=f^+ - f^-$.
Linearly, this allows us to define for any measurable $f$:
\begin{equation*}
\int_H f \diff \mu = \int_H f^+ \diff \mu - \int_H f^- \diff \mu
\end{equation*}


The last issue that remains to be settled is whether such a limit even exists.
For this we can use the sequence of simple functions $\psi_n$ defined by:
\begin{equation*}
	\psi_n = \sum_{k=0}^{n2^n -1} 
		\left[ \left(\frac{k}{2^n}\right)^{\left[ \frac{k}{2^n}, \frac{k+1}{2^n} \right)} \right] 
		+ n^{[n,\infty]}
\end{equation*}
Notationally, this definition is rather heavy but is easily understood geometrically as seen below in
Figure~\ref{fig:simplefunc}.
In turn this allows us to define for any non-negative $f$ the sequence of functions  $\varphi_n = \psi_n \circ f$.
We know that $\varphi_n$ is simple since $\psi_n$ is simple.
And since $\psi_n(x) \leq x$ for all $x$ then we also have $0 \leq \varphi_n \leq f$.
But, most importantly we have $0 \leq f(x) - \varphi_n(x) \leq 2^{-n}$ and 
so $\varphi_n$, uniformly approaches $f$ as $n$ approaches infinity. 
\begin{figure}[ht]
	\caption[Approximations using simple functions]{
	The simple functions $\psi_n$ for $n$ from $1$ to $4$.
	As $n$ goes to infinity, $\psi_n$ comes arbitrarily close to $x$ over the range 0 to $n$.
	}
	\label{fig:simplefunc}
	\centering
	\begin{tikzpicture}[y=1cm, x=2.5cm]
		%psi 4
		\foreach \x in {0,0.0625,...,3.99}
			\draw[fill, color=black!10] (\x,0) rectangle ++ (0.0625, \x);
		\draw[fill, color=black!10] (4,0) rectangle (5,4);
		%psi 3
		\foreach \x in {0,0.125,...,2.9}
			\draw[fill, color=black!20] (\x,0) rectangle ++ (0.125, \x);
		\draw[fill, color=black!20] (3,0) rectangle (5,3);
		%psi 2
		\foreach \x in {0,0.25,...,1.9}
			\draw[fill, color=black!40] (\x,0) rectangle ++ (0.25, \x);
		\draw[fill, color=black!40] (2,0) rectangle (5,2);
		%psi 1
		\foreach \x in {0,0.5,...,0.9}
			\draw[fill, color=black!60] (\x,0) rectangle ++ (0.5, \x);
		\draw[fill, color=black!60] (1,0) rectangle (5,1);
	 	%axis
		\draw(0,0) -- coordinate (x axis mid) (5,0);
	    	\draw (0,0) -- coordinate (y axis mid) (0,5);
	    	%ticks
	    	\foreach \x in {0,1,...,5}
	     		\draw (\x,1pt) -- (\x,-3pt)
				node[anchor=north] {\x};
	    	\foreach \y in {0,1,...,5}
	     		\draw (1pt,\y) -- (-3pt,\y) 
	     			node[anchor=east] {\y}; 
	     	% y= x		
		\draw (0,0) -- (5,5);
		%legend
		\begin{scope}[shift={(5.25,2.5)}] 
			\draw[yshift=4\baselineskip, fill, color=black!60] (0,0) rectangle (0.5,0.2) --++ (0,-0.1)
				node[right, color=black]{$\psi_1$};
			\draw[yshift=3\baselineskip, fill, color=black!40] (0,0) rectangle (0.5,0.2) --++ (0,-0.1) 
				node[right, color=black]{$\psi_2$};	
			\draw[yshift=2\baselineskip, fill, color=black!20] (0,0) rectangle (0.5,0.2) --++ (0,-0.1) 
				node[right, color=black]{$\psi_3$};
			\draw[yshift=\baselineskip, fill, color=black!10] (0,0) rectangle (0.5,0.2) --++ (0,-0.1) 
				node[right, color=black]{$\psi_4$};
			\draw (0,0) -- (0.5,0) node[right] {$id$};
		\end{scope}
	\end{tikzpicture}
\end{figure}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ''HAVING YOUR CAKE AND EATING IT TOO'' EXAMPLE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Example: \emph{Integrating the Irrationals}}

A Lebesgue measurable hybrid set can now have an orientation like Riemann integral intervals 
and the power of Lebesgue integration.
Consider the indicator function for the set of rational numbers, $1_\mathbb{Q}$ which evaluates to 
1 for rational numbers and 0 for irrational numbers.
Over the interval $[0,1]$ on the real line, this is not Riemann-integrable.
Suppose we have some generalized partition of $[0,1]$ made up of intervals.
Then for any of these sub-intervals, there will be at least one rational and one irrational number.
Hence the upper Riemann sum will be 1 and the lower Riemann sum will be 0.
No matter how small we make our intervals, there will always be a rational and irrational number in each.
The two sums will never converge and so there is no well-defined Riemann integral.


On the other hand, it is Lebesgue integrable:
\begin{equation*}
	\int_{[0,1]} 1_\mathbb{Q} \diff\mu = \mu(\mathbb{Q} \cap [0,1])
\end{equation*}
But since $\mathbb{Q}$ is countable, it has measure 0 and so $1_\mathbb{Q}$ \emph{is} Lebesgue integrable on $[0,1]$.
Similarly, we could also find that:
\begin{equation*}
	\int_0^1 1_{\mathbb{R}\setminus \mathbb{Q}} \diff \mu
		= \int_{[0,1]} 1_{\mathbb{R}\setminus \mathbb{Q}} \diff \mu
		= \mu \big( (\mathbb{R}\setminus\mathbb{Q}) \cap [0,1] \big)
		= \mu \big( (\mathbb{R}\setminus\mathbb{Q}) \cap [0,1] \big)
		= 1
\end{equation*}

One would expect the same process to hold for when integrating from 0 to 1 but restricted to sets, this is not possible.
The statement $[1,0] = -[0,1]$ is nonsense.
But if we instead take integrating from $a$ to $b$ to mean the Lebesgue integral over the oriented $[\![a,b]\!]$.
Additionally instead of the Lebesgue measure $\mu$ which is defined for 
measure $\nu$:

\begin{equation*}
	\int_1^0 1_{\mathbb{R}\setminus \mathbb{Q}} \diff \nu
		\;=\; \int_{[\![0,1]\!]} 1_{\mathbb{R}\setminus \mathbb{Q}} \diff \nu
		\;=\; \nu \big( (\mathbb{R}\setminus\mathbb{Q} ) \otimes [\![1,0]\!] \big)
		\;=\; -1
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% DIFFERENTIAL FORMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Differential Forms}

Rather than thinking of integrals as functions over $n$-rectangles, an often more useful language is to use 
\emph{differential forms}.
We define a \textbf{(differential) 0-form} $\beta$ on $\mathbb{R}^n$ as any function 
$\beta : \mathbb{R}^n \to \mathbb{R}$.
And there is very little else to say as they are just functions on $\mathbb{R}^n$.



A \textbf{(differental) 1-form} $\omega$ on $\mathbb{R}^n$ is an expression of the form:
\begin{equation*}
	\omega = f_1(\text{x}) \diff x_1 + f_2(\text{x}) \diff x_2 + \ldots + f_n(\text{x}) \diff x_n
\end{equation*}
Now this look very much like something we're used to integrating.
Specifically it is something that be used as an integrand over a 1 dimensional domain.
For example, Green's theorem is introduced using differential forms without even mentioning them as such.
\begin{equation}
	\tag{Green's Theorem}
	\textcolor{black!40}{
		\int_D \left( \frac{\partial f_2}{\partial x} - \frac{\partial f_1}{\partial y}  \right) \diff x \diff y 
		=\int_{C}
	} f_1(x,y) \diff x + f_2(x,y) \diff y
\end{equation}
Here $C$ is a closed curve that encloses $D$ a region in the $(x,y)$ plane; hence the right-hand side is an integral of
a 1-form over a 1-dimensional curve.
Having multiple $\diff x_i$ appearing in a single integrand may initially seem unusual when first presented, 
but is quite intuitively handled.
Integration is linear so just as we can separate $\int f(x) + g(x) \diff x$ into $\int f(x) \diff x + \int g(x) \diff x$, 
we can similarly breakup an integral of a 1-form into the sum of integrals over \emph{basic} 1-forms 
(1-forms involving a single term).
\begin{equation*}
	\int f_1(\text{x}) \diff x_1 
		+ f_2(\text{x}) \diff x_2 
		+ \ldots 
		+ f_n(\text{x}) \diff x_n
	=	\sum_{i=1}^n \left( \int f_i \diff x_i \right)
\end{equation*}


Adding two 1-forms then is quite straight-forward; simply collect terms with matching $\diff x_i$.
So if we can add differential forms but what about multiplication?
For a 0-form $\beta$ and 1-form $\omega$ as defined above, the answer the answer is a simple yes:
\begin{equation*}
	(\beta \cdot \omega)(\text{x}) 
		= \beta(\text{x})f_1(\text{x}) \diff x_1 
		+ \ldots  
		+ \beta(\text{x})f_n(\text{x}) \diff x_n
\end{equation*}
The result is a 1-form where each basic 1-form term is the product of $f_i$ and $\beta$ in $\mathbb{R}$.
To ``multiply'' two 1-forms together however we must turn instead to the wedge product $\wedge$.

First of all, the wedge product is primarily defined by being \emph{anti-commutative} or \emph{skew-symmetric}.
That is, $\diff x \wedge \diff y = -\diff y \wedge \diff x$ and several results will immediately follow.
When applied to two identical $\diff x$, we have $\diff x \wedge \diff x = - \diff x \wedge \diff x$ 
and so $\diff x \wedge \diff x = 0$.
Additionally, for any permutation $\sigma$ of $[p]$
\begin{equation*}
	\diff x_1 \wedge ... \wedge \diff x_p 
	= \text{sgn}(\sigma) \diff x_{\sigma(1)} \wedge ... \wedge \diff x_{\sigma(p)}
\end{equation*}
The wedge product of two 1-forms moves us out of the realm of 1-forms which have basis $\diff x_i$ and into the 
realm of 2-forms with basis $\diff x_i \wedge \diff x_j$.


\begin{definition}
	Given a $k$-rectangle $\Omega \in \mathbb{R}^n$ with coordinates $\text{x} = (x_1, x_2, \ldots, x_n)$
	A \textbf{differential $p$-form} $\beta$ over $\Omega$ has the form:
	\begin{equation}
		\beta = \sum_{j_1 \in [n]} \ldots \sum_{j_p \in [n]} b_{(j_1, \ldots, j_p)}(\text{x})
				\diff x_{j_1} \wedge \ldots \wedge \diff x_{j_p}
	\end{equation}
	Typically, we will take $j$ to be the vector $(j_1, \ldots, j_p)$ 
	and express $\beta$ instead as a single sum multi-indexed
	by $j$, $\sum_j b_j(x) \; \diff x_{j_1} \wedge \ldots \wedge \diff x_{j_p}$.
	We denote the space of all $p$-forms on $\Omega$ by $\Lambda^p(\Omega)$.
\end{definition}


\begin{definition}
	Let $\alpha = \sum_i a_i(x) \diff x_{i_1} \wedge \ldots \wedge \diff x_{i_p} \in \Lambda^p(\Omega)$ 
	and $\beta = \sum_j b_j(x) \diff x_{j_1} \wedge \ldots \wedge \diff x_{j_q} \in \Lambda^q(\Omega)$. 
	We extend the wedge product to 
	$\wedge : \Lambda^p(\Omega) \times \Lambda^q(\Omega) \to \Lambda^{p+q}(\Omega)$ by:
	\begin{equation}
		\alpha \wedge \beta  = \sum_{i,j} a_i(x) b_j(x)
			\diff x_{i_1} \wedge \ldots \wedge \diff x_{i_p} \wedge 
			\diff x_{j_1} \wedge \ldots \wedge \diff x_{j_q}
	\end{equation}
\end{definition}

Although we take all possible $\binom{n}{q} \cdot \binom{n}{p}$ pairs of 
$a_i (x) \diff x_{i_1} \wedge \ldots \wedge \diff x_{i_p}$ and $b_i (x) \diff x_{i_1} \wedge \ldots \wedge \diff x_{i_p}$,
most of the possible terms will end up being zero.
If \emph{any} of the terms in $\diff x_{i}$ appears in $\diff x_{j}$, then the wedge product will be zero 
and no term will be contributed.
As such, if $q+p > n$, there will be a duplicate in every term and so the entire sum will be zero.
When all is said and done, at most we will have $\binom{n}{p+q}$ terms.
Rather than the skew-symmetry we had when commuting $\diff x \wedge \diff y$, 
in higher dimensions the sign depends on $p \cdot q$ of the $p$-form and $q$-form we are commuting.
Specifically, 
\begin{equation}
	\alpha \wedge \beta = (-1)^{pq} \beta \wedge \alpha
\end{equation}
This can be easily seen by commuting each of $\diff x_{j_1}, \ldots \diff x_{j_q}$ terms each past 
$\diff x_{i_1}\wedge \ldots \wedge \diff x_{i_p}$. 
So we are commuting $q$ terms each past $p$ terms, reversing the sign each time for a net $(-1)^{pq}$.


The wedge product is only one part of our algebra of differential forms.
We have several other nice identities for its behaviour with addition and multiplication.
For the following, we consider $f$ to be a function on $\mathbb{R}^n$.
Additionally we consider the differential forms $\omega_1$ and $\omega_2$ to be $k$-forms, 
$\alpha$ to be a $p$-form and $\beta$ to be an $q$-form.
Then we have the following:
\begin{align}
	(\omega_1 + \omega_2) \wedge \alpha  & \;=\; \omega_1 \wedge \alpha + \omega_2 \wedge \alpha \\
	(\omega_1 \wedge \alpha) \wedge \beta & \;=\; \omega_1 \wedge ( \alpha \wedge \beta ) \\
	(f \cdot \omega_1) \wedge \alpha & \;=\;  f \cdot (\omega_1 \wedge \alpha) \;=\; \omega_1 \wedge (f \cdot \alpha)
\end{align}
These should all be quite obvious from definitions. 
We should also note the identities which are \emph{not} present.
We have defined the sum of $\omega_1$ and $\omega_2$: two differential forms which are the same dimension but not
the sum of $\alpha$ and $\beta$: differential forms with different dimension.
It is clear how one would add two differential forms of the same dimension as both were defined as sums to begin with.
We also do not define the multiplication of $\cdot$ two differential forms but we multiplying a form by a function is simply:
\begin{equation*}
	(f \cdot \alpha) (x) = \sum_i f(x)\cdot a_i(x) \diff x_{i_1} \wedge \ldots \wedge \diff x_{i_p}
\end{equation*}


Integrating over a $k$-form is quite simple.
First we shall consider integrating a $k$-form over a $k$-rectangle in $\mathbb{R}^k$.
Such a $k$-form is also known as a \emph{top-dimensional form}.
As we saw previously, any form of higher degree must be zero.
If $\omega$ is such a top form then we can always write
\begin{equation*}
	\omega = f \; \diff x_1 \wedge \ldots \wedge \diff x_k
\end{equation*}
for some function $f$.
Other presentations of $\omega$ exist, but we can always achieve such a presentation by commuting over $\wedge$ 
to the canonical ordering $x_1, \ldots, x_n$.
Once we have the $k$-form in this presentation, we just remove the wedges and evaluate the integral using the
integrand $f \diff x_1 \diff x_2 \ldots \diff x_k$.

\begin{definition}
	Let $\alpha$ be a $k$-form on $\Omega \subset \mathbb{R}^n$ of the form 
	$\alpha = A(x) \diff x_1 \wedge ... \wedge \diff x_n$.
	If $A \in \mathcal{L}^1 (\Omega , \diff x)$ then we define:
	\begin{equation}
		\int_\Omega \alpha = \int_\Omega A(x) \; \diff x
	\end{equation}
	Where the left-hand side is the integral of a $k$-form and the right-hand side is a Lebesgue integral.
	For any $\beta \in \Lambda^k (\Omega)$ we extend this definition linearly as the sum of integrals.
\end{definition}

Finally, we extend the differential operator $\diff$ to act on forms known as the \textbf{exterior derivative}.
For a function (0-form), it is the 1-form:
\begin{equation}
	\diff f = \sum_i \frac{\partial f}{\partial x_i} \diff x_i
\end{equation}
This will result in equivalent 1-forms regardless of the choice of coordinates $\text{x} = (x_1, \ldots, x_n)$.
For higher dimension forms it will similarly map a $k$-form to a $k+1$-form.
This is done by recursively using the identities for $p$-form $\alpha$ and $q$-form $\beta$.
\begin{align}
	\diff (\alpha \wedge \beta) &= (\diff \alpha) \wedge \beta + (-1)^p \alpha \wedge ( \diff \beta) \\
	\diff (\;\diff ( \alpha )\; ) &= 0
\end{align}
and extending linearly for all $k$-forms.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% SINGULAR CUBE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Singular Cubes}

Up until now we have been dealing with the very small set of axis aligned $k$-rectangles 
which is a very limiting class to be restricted to.
Instead we would like to be able to integrate over $k$-rectangles that are deformed by some smooth function.
So assume that we have $X \subset \mathbb{R}^m$ and $Y \subset \mathbb{R}^n$ and a smooth map 
 $\varPhi : X \to Y$.
Not only can we map points from $X$ to points in $Y$ but we can \emph{push foward} vectors from $X$ to vectors in $Y$
and with them, push forward tangent spaces as well.


\begin{definition}
	We denote the \textbf{standard $\boldsymbol{k}$-cube} as the specific $k$-rectangle $[\![0,1]\!]^k$ in 	
	$\mathbb{R}^n$ which is the Cartesian product of $k$ copies of $[\![0,1]\!]$.
	We also consider $[\![0,1]\!]^0 = \{0\}$.
	Given an $k$-dimensional manifold $M$, a \textbf{singular $\boldsymbol{k}$-cube in $\boldsymbol{M}$} is a 
	smooth differentiable map $c$ from the standard $k$-cube to $M$, $c:[\![0,1]\!]^k \to M$.
	We will abuse this notation somewhat by also using $c \subseteq M$ to refer to the image of $[\![0,1]\!]^k$ under $c$.
\end{definition}

For example, the hemisphere $H = \{ (x,y) \; | \: x \geq 0, y\geq 0, x^2+y^2 \leq 1 \}$
is a singular 2-cube under the transformation $c: (r, \varphi) \mapsto r \cos(\pi \varphi) x+ r \sin(\pi \varphi) y$.
Depending on the context we might refer to either $H$ or $c$ as a singular cube.
Also, the choice of using specifically the standard $k$-cube is arbitrary.
A differentiable map $f$ from $[\![a,b]\!]$ can always be composed with $g:t \mapsto ta +(1-t)b$ 
to construct the singular cube $c=f \circ g$.



However if we have a valid integral $\int_{\Omega} \omega$ with $\Omega$ in some space $X$, 
if we push forward $\Omega$ by some function $c$ to another space $Y$, 
then the integral $\int_{c(\Omega)} \omega$ is no longer valid.
The differential form $\omega$ was expressed in coordinates for $X$ but now that the domain of the integral is in $Y$,
we must perform a change of coordinates.
The true reason why we use differential forms is how cleanly they handle this change in coordinates 
through the use of pull-backs.

Informally, a pullback is a \emph{reversed} function composition.
In typical function composition $(f \circ g)(x) = f(g(x))$, for input $x$ one first evaluates the second function $g$ at $x$
before feeding the result of $g(x)$ into the first function $f$.
The pre-composition or pullback would be $f^*g = g(f(x))$. 
One first evaluates the first function and feeds the result into the second.
Gets its name from pulling $f$ back through $g$.
Using the following identities:
\begin{align}
	F^* (\alpha \wedge \beta ) &= (F^* \alpha) \wedge (F^* \beta) \\
	F^* (\diff \beta ) &= \diff F^* \beta
\end{align}
one finds a very convenient way to express change of basis inside an integral.

\begin{theorem}
Let $F : X  \to \Omega$ be an (orientation-preserving diffeomorphism) and $\alpha$ an integrable $n$-form on $\Omega$ then
\begin{equation}
\int_{F(X)} \alpha = \int_X F^* \alpha
\end{equation}
\end{theorem}



To integrate over a manifold $M$, we first observe that each local chart $U_i$ in the manifold is essentially a singular cube.
If the chart is not a map over the standard cube, then there exists a diffeomorphism 
that can be composed with the local map to transform it into a singular cube.
It is then a matter of stitching together these local charts so that points in the manifold are not ``double-counted''.
To do this we use a \textbf{partition of unity} on $M$.
That is, a collection of functions $\{ \psi_i \}_i$, $\psi_i: U_i \to \mathbb{R}$ which is:
\begin{description}
	\item[Non-negative:] For each $\psi_i$ and for all $x \in \text{supp}(\psi_i)$, $\psi_i \geq 0$ .
	\item[Sums to one(unity):] For all $x \in M$, $\sum_i \psi_i(x) = 1$
	\item[Locally finite:] for any point in $M$ there are only a finite number of non-zero $\psi_i$
\end{description}
Given such a parititon of unity for the manifold, we define the integral over all of $M$ as:
\begin{equation}
\int_M \alpha = \sum_i \int_{U_i} \psi_i \alpha
\end{equation}








