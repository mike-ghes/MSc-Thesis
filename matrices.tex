\chapter{Symbolic Block Linear Algebra}

In mathematics literature, is common practice to represent matrices as being broken up into blocks or submatrices.
For example the matrix $M$ is a block matrix with submatrices $A,B,C,D$.
\begin{equation}
	M= \left[
		\begin{array}{c|c}
			A & B \\
			\hline
			C & D \\
		\end{array}
	\right]
	= \left[
		\begin{array}{ccc|ccc}
			A_{1,1} & \ldots &A_{1,m} & B_{1,1} & \ldots & B_{1,q} \\
			\vdots & & \vdots & \vdots & & \vdots \\
			A_{n, 1} & \ldots & A_{n, m} & B_{n,1} & \ldots & B_{n,q} \\
			\hline
			C_{1, 1} & \ldots & C_{1, m} & D_{1,1} & \ldots & D_{1,q} \\
			\vdots & & \vdots & \vdots & & \vdots \\
			C_{r,1} & \ldots & C_{r,m} & D_{r,1} & \ldots & D_{r,q} \\
		\end{array}
	\right]
\end{equation}
The result is an $(n+r) \times (m+q)$ matrix.
Within a block, there is a one-to-one correspondance from the entries of $M$ to the entries of a submatrix (shifted by some offset).
In the above example, elements of $B$ would have an offset of $(0,m)$ since $B_{i,j}$ corresponds with $M_{i+0, j+m}$

An important condition however is that these partitions are divided by \emph{unbroken} horizontal and vertical lines.
So for example the height of $A$ must be the same as the height of $B$ 
and the width of $A$ must be the same as the width of $C$.

\todo[inline]{Many blocks}

Clearly block matrices are at the very least a convenient notation but they also have considerable practical applications as well.
For example when multiplying large matrices, block matrices can be used to improve cache complexity. \cite{lam1991cache}
In some cases, when a submatrices are known to have a nice properties, many optimizations can arise.
For example inverting a \emph{block diagonal matrix} can be performed by inverting each block individually. 
\begin{equation}
	\begin{bmatrix}
		A_1 & 0 & \ldots & 0 \\
		0 & A_2 & \ddots & \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		0 & \ldots & 0 & A_n \\
	\end{bmatrix}^{-1}
	=
	\begin{bmatrix}
		{A_1}^{-1} & 0 & \ldots & 0 \\
		0 & {A_2}^{-1} & \ddots & \vdots \\
		\vdots & \ddots & \ddots & 0 \\
		0 & \ldots & 0 & {A_n}^{-1} \\
	\end{bmatrix}
\end{equation}

Although block matrices are commonly used with fixed values for bounds.
Computer algebra has unsatisfactory structures to represent these when the bounds between blocks are symbolic.
For example, the sum of $2 \times 2$ block matrices, naively leads to 9 possible cases of overlapping regions depending on
the relationship between horizontal and vertical boundaries between blocks.


\begin{figure}[ht]
	\caption[Possible block overlaps of $2 \times 2$ block matrices.]{There are 9 possible permutations. 1x (a) 2x (b) 2x (c) 4x (d)}
	\begin{subfigure}[b]{0.24\textwidth}
		\caption{}
		\begin{equation*}
			\left[ \begin{array}{c|c}
				\; & \; \\
				\hline
				& \\
			\end{array}\right]
		\end{equation*}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\textwidth}
		\caption{}
		\begin{equation*}
			\left[ \begin{array}{c|c}
				\; & \; \\
				\hline
				& \\
				\hline
				& \\
			\end{array}\right]
		\end{equation*}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\textwidth}
		\caption{}
		\begin{equation*}
			\left[ \begin{array}{c|c|c}
				\; & \; & \;  \\
				\hline
				& \\
			\end{array}\right]
		\end{equation*}
	\end{subfigure}
	\begin{subfigure}[b]{0.24\textwidth}
		\caption{}
		\begin{equation*}
			\left[ \begin{array}{c|c|c}
				\; & \; & \; \\
				\hline
				& \\
				\hline
				& \\
			\end{array}\right]
		\end{equation*}
	\end{subfigure}
\end{figure}


In this chapter we will show a method using hybrid functions to avoid this case based approach for both addition
and multiplication of matrices.
First we introduce some notation that will be used frequently over the next 3 chapters.

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTERVALS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Oriented Intervals}

\begin{definition}
	Given a totally ordered set $(X, \leq)$ \emph{(and with an implied strict ordering $<$)}, 
	for any $a,b \in X$, an \textbf{interval between $\boldsymbol{a}$ and $\boldsymbol{b}$} 
	is the set of elements in $X$ between $a$ and $b$, up to inclusion of $a$ and $b$ themselves. 
	Formally:
	\begin{equation} 
	\begin{array}{cc}
		{[a,b]}_X =  \{ x \in X \;|\; a \leq x \leq b \} \\
		{[a,b)}_X =  \{ x \in X \;|\; a \leq x < b \} \\
		{(a,b]}_X =  \{ x \in X \;|\; a < x \leq b \} \\
		{(a,b)}_X =  \{ x \in X \;|\; a < x < b \}
	\end{array} 
	\end{equation}
	When context makes $X$ obvious or the choice of $X$ is irrelevant, we shall omit the subscript.
\end{definition}

It should be noted that when $b$ is less than $a$, $[b,a]$ is the empty set. 
In terms of idempotency, the bounds determine whether or not an interval will be empty.
$[a,a]$ which contains $a$ and al points equivalent to $a$ while $(a,a)$, $(a,a]$, and $[a,a)$ are all empty sets.
As intervals are simply sets, they can naturally be interpreted as hybrid sets.
If $a \leq b \leq c$, for intervals then we have $[a,b) \oplus [b,c) = [a,c)$.
In this case, $\oplus$ seems to behave like concatenation but this is not always true.
If instead we had $a \leq c \leq b$ then $[a,b) \oplus [b,c) = [a,b)$.

\begin{equation}
	[a,b) \oplus [b,c) =
	\begin{cases}
		[a,c) & a \leq b \leq c \\
		[a,b) & a \leq c \leq b \\
		[b,c) & b \leq a \leq c \\
		\emptyset & \text{otherwise} \\
	\end{cases}
\end{equation}

One could alternatively write $[a,b)\oplus [b,c) = [\; \min(a,b),\max(b,c) \;)$ but this simply sweeps the problem 
under the rug.
When working with intervals, a case-based approach to consider relative ordering of endpoints easily becomes quite cumbersome.
Thus we turn to oriented intervals.


\begin{definition}
	We define \textbf{oriented intervals} with $a,b\in X$, where $X$ is a totally ordered set, 
	using hybrid set point-wise subtraction as follows:
	\begin{equation}
		\begin{array}{cc}
			{[\![ a,b )\!)} = [a,b) \ominus [b,a) \\
			{(\!( a,b ]\!]} = (a,b] \ominus (b,a] \\
			{[\![ a,b ]\!]} = [a,b] \ominus (b,a) \\
			{(\!( a,b )\!)} = (a,b) \ominus [b,a]
		\end{array}
	\end{equation}
\end{definition}

For any choice of \emph{distinct} $a$ and $b$, exactly one term will be empty; there can be no ``mixed'' multiplicities from a single oriented interval.
Unlike traditional interviews where $[a,b)$ would be empty if $b < a$,  
the oriented interval $[\![a,b)\!)$ will have elements with negative multiplicity.
Several results follow immediately from this definition.

\begin{theorem} For all $a,b,c \in \mathbb{R}$, 
	\begin{equation}
		\begin{array}{cc}
		{[\![a,b)\!)} = \ominus [\![b,a)\!) \\
		{(\!(a,b]\!]} = \ominus (\!(b,a]\!] \\
		{[\![a,b]\!]} = \ominus (\!(a,b)\!) \\
		{(\!(a,b)\!)} = \ominus [\![a,b]\!]
		\end{array}
	\end{equation}
\end{theorem}

We should make a note here how oriented intervals behave when $a=b$.
Like their unoriented analogues, the oriented intervals $[\![ a,a )\!)$ and $(\!( a,a ]\!]$ are still both empty sets.
The interval $[\![a,a]\!]$ still contains points equivalent to $a$ (with multiplicity 1).
However, unlike traditional intervals $(\!(a,a)\!)$ is \emph{not} empty but rather, $(\!(a,a)\!) = \ominus [\![a,a]\!]$ and so contains all points equivalent to $a$ but with a multiplicity of $-1$.
The advantage of using oriented intervals is that now $\oplus$ does behave like contatenation.

\begin{theorem}
	For all $a,b,c \in \mathbb{R}$ (regardless of relative ordering),
	\begin{equation}
		[\![ a,b )\!) \oplus [\![ b,c )\!) = [\![ a,c )\!)
	\end{equation}
\end{theorem}

\begin{proof}
	Following from definitions we have:
	\begin{align*}
		[\![a,b)\!) \oplus [\![ b,c )\!)
		& = \left( [a,b) \ominus [b,a) \right) \oplus \left( [b,c) \ominus [c,b) \right)\\
		& = \left( [a,b) \oplus [b,c) \right) \ominus \left( [c,b) \oplus [b,a) \right)
	\end{align*}
	\begin{description}
		\item[Case 1: $a \leq c$] then $[c,a) = \emptyset$ and so $[\![a,c)\!) = [a,c)$. 
		\begin{description}
			\item[Case 1.a: $a \leq b \leq c$] then $[c,b) = [b,a) = \emptyset$ and $[a,b) \oplus [b,c) = [a,c)$
			\item[Case 1.b: $b \leq a \leq c$] then $[b,c) \ominus [b,a) = [b,a) \oplus [a,c) \ominus [b,a) = [a,c)$
			\item[Case 1.c: $a \leq c \leq b$] then $[a,b) \ominus [c,b) = ([a,c) \oplus [c,b)) \ominus [c,b) = [a,c)$
		\end{description}
		\item[Case 2: $c < a$] then $[a,c) = \emptyset$ and so $[\![a,c)\!) = \ominus [c,a)$. 
		\begin{description}
			\item[Case 2.a: $c \leq b \leq a$] 
				then $[a,b) = [b,c) = \emptyset$ and $\ominus[c,b) \ominus [b,a) = \ominus [c,a)$
			\item[Case 2.b: $b \leq c \leq a$] 
				then $\ominus [b,a) \oplus [b,c) = \ominus([b,c) \oplus [c,a)) \oplus [b,c) = \ominus[c,a)$
			\item[Case 2.c: $c \leq a \leq b$] 
				then $\ominus [c,b) \oplus [a,b) = \ominus([c,a) \oplus [a,b)) \oplus [a,b) = \ominus[c,a)$
		\end{description}
	\end{description}
\end{proof}

This sort of reasoning is routine but a constant annoyance when dealing with intervals 
and is exactly the reason we want to be working with oriented intervals.
But now that the above work is done, we can use oriented intervals 
and not concern ourselves with the relative ordering of points.
Many similar formulations such as $[\![ a,b ]\!] \oplus (\!( b,c )\!) = [\![a,c)\!)$ or $(\!(a,b)\!) \oplus [\![b,c)\!) = (\!(a,c)\!)$ 
are also valid for any ordering of $a,b,c$ by an identical argument. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% VECTORS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Vector Addition}
Addition for partitioned vectors and $2 \times 2$ matrices using hybrid functions has already been considered in \cite{carette2010}.
The method is nearly identical to that of adding piecewise functions.
In fact, one could think of both as simply addition of piecewise functions over a subset of 
$\mathbb{N}$ and $\mathbb{N} \times \mathbb{N}$ respectively.
However it will provide a good example of oriented intervals in use and as an introduction to multiplication of 
symbolic block matrices.


First we will consider the addition of two $n$-dimensional vectors.
Addition of two vectors: $U= (u_1, u_2, \ldots u_n$ and $V = (v_1, v_2, \ldots, v_n)$ is itself an $n$ dimensional vector 
defined as:
\begin{equation}
	U+V = (u_1+v_1, \; u_2+v_2, \ldots, u_n+v_n)
\end{equation}


In particular, we would like to consider the addition of vectors $U$ and $V$ which are each partitioned into two intervals,
$[1,k]$ and $(k,n]$ as well as $[1,\ell]$ and $(\ell, n]$.
Over each interval, taking the value of different functions, as in:
\begin{align}
	U &= [ u_1, u_2, \ldots, u_{k}, u'_1, u'_2, \ldots, u_{n-k} ] \\
	V &= [ v_1, v_2, \ldots, v_{\ell}, v'_1, v'_2, \ldots, v_{n-\ell} ]
\end{align}


These can be written more concisely as hybrid functions over intervals. 
Using intervals, these vectors can be represented by hybrid functions over their indices.
For example
\begin{align}
	U &= (i \mapsto u_i)^{[\![1, k]\!]} \oplus (i \mapsto u_{i-k})^{(\!(k,n]\!]} \\
	V &= (i \mapsto v_i)^{[\![1, \ell]\!]} \oplus (i \mapsto v_{i-\ell})^{(\!(\ell,n]\!]}
\end{align}
Although for clarity and succinctness we will use $(u_i)$ instead of $(i \mapsto u_i)$.
\begin{align}
	U &= (u_i)^{[\![1, k]\!]} \oplus (u_{i-k})^{(\!(k,n]\!]} \\
	V &= (v_i)^{[\![1, \ell]\!]} \oplus v_{i-\ell})^{(\!(\ell,n]\!]}
\end{align}


To add $U$ and $V$
\begin{align}
	U + V
	&= \left( (u_i)^{[\![1, k]\!]} \oplus (u'_{i-k})^{(\!(k,n]\!]} \right) 
		+
		\left( (v_i)^{[\![1, \ell]\!]} \oplus (v'_{i-\ell})^{(\!(\ell,n]\!]} \right) \\
	&= \left( (u_i)^{[\![1, k]\!]} \oplus (u'_{i-k})^{(\!(k,\ell]\!]} \oplus (u'_{i-k})^{(\!(\ell,n]\!]} \right) 
		+
		\left( (v_i)^{[\![1, k]\!]} \oplus (v_i)^{(\!(k, \ell]\!]} \oplus (v'_{i-\ell})^{(\!(\ell,n]\!]} \right) \\
	&= \R[+] \left( (u_i + v_i)^{[\![1, k]\!]} 
		\oplus (u'_{i-k} + v_i)^{(\!(k,\ell]\!]} 
		\oplus (u'_{i-k}+v'_{i-\ell})^{(\!(\ell,n]\!]} \right)
\end{align}


The choice to partition $[\![1,n]\!]$ into $[\![1,k]\!] \oplus (\!(k,\ell]\!] \oplus (\!(\ell, n]\!]$ is only one common refinement.
We can just as easily use $[\![1,\ell]\!] \oplus (\!(\ell, k]\!] \oplus (\!(k, n]\!]$ to get the equivalent expression:
\begin{equation}
	U + V = \R[+] \left( (u_i + v_i)^{[\![1, \ell]\!]} 
		\oplus (u_{i} + v'_{i-\ell})^{(\!(\ell,k]\!]} 
		\oplus (u'_{i-k}+v'_{i-\ell})^{(\!(k,n]\!]} \right)
\end{equation}


We must be careful while evaluating these expressions to not forget that $(u'_{i-k} + v_i)$ 
is actually shorthand for the function:
\begin{equation*}
	(u'_{i-k} + v_i) = (i \mapsto u'_{i-k}) + (i \mapsto v_i) = (i \mapsto u'_{i-k} + v_i)
\end{equation*}
As a function, it may not be evaluable over the entire range implied in a given term.
The same lambda-lifting trick of using pseudo-functions as in the previous section easily solves this.


For example, consider the concrete example where $n=5$, $k=4$ and $\ell = 1$ so that
$U = [ u_1, u_2, u_3, u_4, u'_1 ]$ and
$V = [ v_1, v'_1, v'_2, v'_3, v'_4 ]$.
We will also only assume that the functions $u_i, u'_i, v_i$ and $v'_i$ are defined only on the intervals in which they appear (e.g. $u_5$ is undefined, as is $v'_1$).
Then we have:
\begin{equation*}
	U + V = (u_i + v_i)^{[\![1,4]\!]} \oplus (u'_{i-4} + v_i)^{(\!(4,1]\!]} \oplus (u'_{i-4} + v'_{i-1})^{(\!(1,5]\!]}
\end{equation*}

None of the individual subterms cannot be evaluated directly.
In the first term, $v_i$ is not totally defined over the interval $[\![1,4]\!]$.
In the third term, on the interval $(\!(1,5]\!]$, $u'_{i-4}$ would even evaluated on negative indices.
However, these unevaluable terms also appear in the middle term however the interval $(\!(4,1]\!]$ is a negatively oriented
 interval and the offending points cancel exactly as in the previous chapter.
\begin{align*}
	U + V
		&= (u_i + v_i)^{[\![1,1]\!] \oplus (\!(1,4]\!]} 
			\oplus (u'_{i-4} + v_i)^{\ominus(\!(1,4]\!]} 
			\oplus (u'_{i-4} + v'_{i-1})^{(\!(1,4]\!] \oplus (\!(4,5]\!]}\\
		&= (u_i + v_i)^{[\![1,1]\!]} 
			\oplus \left((u_i + v_i) - (u'_{i-4} + v_i) + (u'_{i-4} + v'_{i-1})\right)^{[\![1,4]\!]} 
			\oplus (u'_{i-4} + v'_{i-1})^{(\!(4,5]\!]} \\ 
		&= (u_i + v_i)^{[\![1,1]\!]} 
			\oplus (u_i + v'_{i-1})^{(\!(1,4]\!]} 
			\oplus (u'_{i-4} + v'_{i-1})^{(\!(4,5]\!]}
\end{align*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Matrix Addition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Addition}


First we will consider the addition of two $n \times m$ block matrices $A$ and $B$ of the form:
\begin{equation}
	A = \left[ \begin{array}{cc} A_{11} & A_{12} \\ A_{21} & A_{22} \end{array} \right]
	\;\;\;\;\; \text{and} \;\;\;\;\;
	B = \left[ \begin{array}{cc} B_{11} & B_{12} \\ B_{21} & B_{22} \end{array} \right]
\end{equation}
Since these are block matrices then $A_{ij}$ and $B_{ij}$ are not entries but sub matrices themselves.
We shall assume that $A_{11}$ is a $(q \times r)$ matrix and $B_{11}$ is a $(s \times t)$ matrix.

Also use $A_{11}$ to denote the region $A_{11}$ is defined over.

Rely on context; full-sized symbol is function, superscript is region.

\begin{equation}
	A = A_{11}^{A_{11}} \oplus A_{12}^{A_{12}} \oplus A_{21}^{A_{21}} \oplus A_{22}^{A_{22}} 
\end{equation}
\begin{equation}
	B = B_{11}^{B_{11}} \oplus B_{12}^{B_{12}} \oplus B_{21}^{B_{21}} \oplus B_{22}^{B_{22}} 
\end{equation}

The sum of 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Matrix Multiplication
%
% q = k1, r = q2, s = ell1, t=ell2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Multiplication}



Again, we will assume $2 \times 2$ block matrices $A$ and $B$.
However for these matrices to be conformable for multiplication they must be  $n \times m$ and $m \times p$ rather than
both $n \times m$.
\begin{equation}
	A = \left[ \begin{array}{cc} A_{11} & A_{12} \\ A_{21} & A_{22} \end{array} \right]
	\;\;\;\;\; \text{and} \;\;\;\;\;
	B = \left[ \begin{array}{cc} B_{11} & B_{12} \\ B_{21} & B_{22} \end{array} \right]
\end{equation}
Where $A_{11}$ is a $q \times r$ matrix and $B_{11}$ is a $s \times t$ matrix.
Note that $0 \leq r , s \leq m$ but the ordering of $r$ and $s$ is unknown.

The simplest case is for $r=s$.
In  this case, 4 regions will arise.
\begin{equation}
	AB = \left[ \begin{array}{cc} 
		A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\ 
		A_{21}B_{11}+A_{22}B_{21} & A_{21}B_{12}+A_{22}B_{22}
	\end{array} \right]
\end{equation}


For the general case ($k_2 \neq \ell_1$), we still get back a $2 \times 2$ block matrix, which we will denote $C$.
\begin{equation}
	AB = \left[ \begin{array}{cc} C_{11} & C_{12} \\ C_{21} & C_{22} \end{array} \right]
\end{equation}
$C_{11}$ is a $k_1 \times \ell_2$ sub-matrix; the sizes of the other partitions can be derived from this.
The domains of these 12 submatrices can be partitioned using:
\begin{equation*}\begin{array}{cc}
	N_1 = [\![0,k_1)\!) & N_2 = [\![k_1, n)\!) 
\end{array}\end{equation*}
\begin{equation*}\begin{array}{cc}
	P_1 = [\![0, \ell_2)\!) & P_2 = [\![ \ell_2, p)\!)
\end{array}\end{equation*}
\begin{equation*}\begin{array}{ccc}
	M_1 = [\![0,k_2)\!) & M_2 = [\![ k_2, \ell_1)\!) & M_3 = [\![ \ell_1, m)\!)
\end{array}\end{equation*}
So we can rewrite $A$ and $B$ as:
\begin{equation}
	A = 	A_{11}^{N_1 \times M_1} \oplus A_{12}^{N_1 \times (M_2 \oplus M_3)} \oplus 
			A_{21}^{N_2 \times M_1} \oplus A_{22}^{N_2 \times (M_2 \oplus M_3)}
\end{equation}
\begin{equation}
	B = 		B_{11}^{(M_1 \oplus M_2) \times P_1} \oplus B_{12}^{(M_1 \oplus M_2) \times P_2} \oplus 
			B_{21}^{M_3 \times P_1} \oplus B_{12}^{M_3 \times P_2}
\end{equation}

Here $\oplus$ is \emph{not} the direct sum of matrices, but hybrid function pointwise sum. 
$\times$ is the Cartesian product of intervals. 

For $i,j \in \{ 1,2 \}$ we can write blocks in $C$ as:
\begin{equation}
	C_{i,j} 	= A_{i,1}^{N_i \times M_1} B_{1,j}^{M_1 \times P_j} 
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3} B_{2,j}^{M_3 \times P_j}
\end{equation}

If $k_2 = \ell_1$ then $M_2 = \emptyset$.
We can think of multiplying a $n \times 0$ matrix by a $0 \times p$ matrix as giving a $n \times p$ matrix which is the sum
over an empty set, 0 everywhere.

If $k_2 < \ell_1$ then this is like treating $A$ and $B$ instead as $2 \times 3$ and $3 \times 2$ block matrices.
$M_1$, $M_2$ and $M_3$ are all positive intervals and form conformable blocks.

If $k_2 > \ell_1$ then $M_2$ is a negative interval which doesn't have a good interpretation.
To simplify $C_i,j$, we can use the partition $ \{ M_1 \oplus M_2, \ominus M_2, M_2 \oplus M_2 \}$ which contains only
positive intervals.


First we rewrite the hybrid functions in the first and third terms:
\begin{align}
	C_{i,j} 	= &\left( A_{i,1}^{N_i \times M_1 \oplus M_2} \oplus A_{i,1}^{N_i \times \ominus M_2} \right)
				\left( B_{1,j}^{M_1 \oplus M_2 \times P_j} \oplus B_{1,j}^{\ominus M_2 \times P_j} \right)
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j} \notag\\
			&+ \left( A_{i,2}^{N_i \times \ominus M_2} \oplus A_{i,2}^{N_i \times M_3 \oplus M_2} \right)
			 \left( B_{2,j}^{\ominus M_2 \times P_j} \oplus B_{2,j}^{M_3 \oplus M_2 \times P_j} \right)
\end{align}

Which are now the product of conformable $1 \times 2$ and $2 \times 1$ block matrices.
This gives us a $1 \times 1$ block matrix :
\begin{align}
	C_{i,j} 	= & A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j}
			+ A_{i,1}^{N_i \times \ominus M_2} B_{1,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j} \notag\\
			&+ A_{i,2}^{N_i \times \ominus M_2} B_{2,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j}
\end{align}

Then we merge the middle three terms:
\begin{align}
	C_{i,j} 	=& \left( A_{i,1}^{N_i \times \ominus M_2} 
				\oplus  A_{i,2}^{N_i \times M_2} 
				\oplus A_{i,2}^{N_i \times \ominus M_2} \right)
			\left( B_{1,j}^{\ominus M_2 \times P_j} 
				\oplus B_{1,j}^{M_2 \times P_j} 
				\oplus B_{2,j}^{\ominus M_2 \times P_j} \right)\notag\\
			&+ A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j} 
\end{align}


$A_{i,2}$ occurs twice with opposite sign on the same region and so cancels itself. As does $B_{1,j}$:
\begin{align}
	C_{i,j} 	=& \left( A_{i,1}^{N_i \times \ominus M_2} \oplus  A_{i,2}^{N_i \times \emptyset}  \right)
			\left(  B_{2,j}^{\ominus M_2 \times P_j} \oplus B_{1,j}^{\emptyset \times P_j} \right)\notag\\
			&+ A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j} 
\end{align}
And functions over empty domains can be removed:

\begin{align}
			= & A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,1}^{N_i \times \ominus M_2} B_{2,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j}
\end{align}


Formally this relies on the following identities:

\begin{equation}
	M^B \oplus M^C = M^{B\oplus C}
\end{equation}
(we can split up a matrix into blocks)

\begin{equation}
	M_1^A \oplus M_2^\emptyset = M_1^A = M_2^\emptyset \oplus M_1^A
\end{equation}
(empty blocks can be ignored)

\begin{equation}
	M_1^{A \times B} M_2^{B \times D} + M_3^{A \times C} M_4^{C \times D} 
	= \left(M_1^{A\times B} \oplus M_3^{A \times C} \right) \left( M_2^{B \times D} \oplus M_4^{C \times D} \right)
\end{equation}





















