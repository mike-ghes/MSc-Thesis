\chapter{Symbolic Block Linear Algebra}

In mathematics literature, is common practice to represent matrices with symbolic blocks such as:
\begin{equation}
	M= \left[
		\begin{array}{c|c}
			A & B \\
			\hline
			C & D \\
		\end{array}
	\right]
\end{equation}
where $A,B,C,D$ are matrices themselves.
Additionally it is assumed that these submatrices agree. 

If $M$ is $n \times m$ and  $A$ is $k \times \ell$ then the sizes of $B,C,D$ are all determined:
They must be $k \times m - \ell$, $n-k \times \ell$ and $n-k \times m-\ell$ respectively.


Block matrices are very important not only from a theoretical perspective but also provide considerable improvements
to matrix multiplication algorithms by improving cache complexity.


Although block matrices are commonly used with fixed values for bounds.
Computer algebra has unsatisfactory structures to represent these when the bounds between blocks are symbolic.


\todo[inline]{more}


\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% INTERVALS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Oriented Intervals}

\begin{definition}
	Given a totally ordered set $(X, \leq)$ \emph{(and with an implied strict ordering $<$)}, 
	for any $a,b \in X$, an \textbf{interval between $\boldsymbol{a}$ and $\boldsymbol{b}$} 
	is the set of elements in $X$ between $a$ and $b$, up to inclusion of $a$ and $b$ themselves. 
	Formally:
	\begin{align}
		[a,b] = \{ x \in X \;|\; a \leq x \leq b \} \\
		[a,b) = \{ x \in X \;|\; a \leq x < b \} \\
		(a,b] = \{ x \in X \;|\; a < x \leq b \} \\
		(a,b) = \{ x \in X \;|\; a < x < b \}
	\end{align}
\end{definition}

It should be noted that for $b<a$, $[b,a]$ is the empty set. 
Also, the interval $[a,a]$ contains a single point while $(a,a)$, $(a,a]$, and $[a,a)$ are all empty.
As intervals are simply sets, they can naturally be interpreted as hybrid sets.
If $a \leq b \leq c$, for intervals $[a,b)$ and $[b,c)$ using the hybrid set operator $\oplus$, one has
$[a,b) \oplus [b,c) = [a,c)$
In this case, $\oplus$ behaves like concatenation but this is not always true.
When $a \leq c \leq b$ then $[a,b) \oplus [b,c) = [a,b)$.
When working with intervals, a case-based approach to consider relative ordering of endpoints easily becomes quite cumbersome.
Thus we turn to oriented intervals.
\todo[inline]{1 more line}

\begin{definition}
	We define \textbf{oriented intervals} with $a,b\in X$, where $X$ is a totally ordered set, 
	using hybrid set point-wise subtraction as follows:
	\begin{align}
		[\![ a,b )\!) = [a,b) \ominus [b,a) \\
		(\!( a,b ]\!] = (a,b] \ominus (b,a] \\
		[\![ a,b ]\!] = [a,b] \ominus (b,a) \\
		(\!( a,b )\!) = (a,b) \ominus [b,a]
	\end{align}
\end{definition}

For any choice of distinct $a$ and $b$, exactly one term will be empty.
Unlike traditional interviews where $[a,b)$ would be empty if $b < a$,  
the oriented interval $[\![a,b)\!)$ simply has elements with negative multiplicity.
Several results follow immediately from this definition.

\begin{theorem} For all $a,b,c \in \mathbb{R}$, 
	\begin{align}
		[\![a,b)\!) &= \ominus [\![b,a)\!) \\
		(\!(a,b]\!] &= \ominus (\!(b,a]\!] \\
		[\![a,b]\!] &= \ominus (\!(a,b)\!) \\
		(\!(a,b)\!) &= \ominus [\![a,b]\!]
	\end{align}
\end{theorem}

We should make a note here how oriented intervals behave when $a=b$.
Like their unoriented analogues, the oriented intervals $[\![ a,a )\!)$ and $(\!( a,a ]\!]$ are both empty 
while $[\![a,a]\!]$ contains the point at $a$ (with multiplicity 1).
However, unlike traditional intervals $(\!(a,a)\!)$ is \emph{not} empty but rather, $(\!(a,a)\!) = \ominus [\![a,a]\!] = \hset{a^{-1}}$.

\todo[inline]{more}

\begin{theorem}
	For all $a,b,c \in \mathbb{R}$ (regardless of relative ordering),
	\begin{equation}
		[\![ a,b )\!) \oplus [\![ b,c )\!) = [\![ a,c )\!)
	\end{equation}
\end{theorem}

\begin{proof}
	$[\![a,b)\!) \oplus [\![ b,c )\!)$ 

	$= \left( [a,b) \ominus [b,a) \right) \oplus \left( [b,c) \ominus [c,b) \right)$ 

	$= \left( [a,b) \oplus [b,c) \right) \ominus \left( [c,b) \oplus [b,a) \right)$

	If $a \geq c$ then $[c,a) = \emptyset$ and so $[\![a,c)\!) = [a,c)$. 
	\begin{description}
		\item[Case 1: $a \leq b \leq c$] then $[c,b) = [b,a) = \emptyset$ and $[a,b) \oplus [b,c) = [a,c)$
		\item[Case 2: $b \leq a \leq c$] then $[b,c) \ominus [b,a) = [b,a) \oplus [a,c) \ominus [b,a) = [a,c)$
		\item[Case 3: $a \leq c \leq b$] then $[a,b) \ominus [c,b) = ([a,c) \oplus [c,b)) \ominus [c,b) = [a,c)$
	\end{description}
	Similar arguments will show that when $c \geq a$, that $[\![a,b)\!) \oplus [\![ b,c )\!) = \ominus [a,c)$.
	
\end{proof}

This sort of reasoning is routine but a constant annoyance when dealing with intervals and is exactly the reason we want to be working with oriented intervals.
Many similar formulations such as $[\![ a,b ]\!] \oplus (\!( b,c )\!) = [\![a,c)\!)$ are also valid for any ordering of $a,b,c$.
We will not enumerate all possible cases here.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% VECTORS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Addition}

\todo[inline]{Vectors as hybrid functions}

We will use the following $n$-dimensional vectors as a running example in this section:

\begin{align}
	U^T &= [ u_1, u_2, \ldots, u_{k}, u'_1, u'_2, \ldots, u_{n-k} ] \\
	V^T &= [ v_1, v_2, \ldots, v_{\ell}, v'_1, v'_2, \ldots, v_{n-\ell} ]
\end{align}

Using intervals, these vectors can be represented by hybrid functions over their indices.

For example
\begin{align}
	U^T &= (i \mapsto u_i)^{[\![1, k]\!]} \oplus (i \mapsto u_{i-k})^{(\!(k,n]\!]} \\
	V^T &= (i \mapsto v_i)^{[\![1, \ell]\!]} \oplus (i \mapsto v_{i-\ell})^{(\!(\ell,n]\!]}
\end{align}
Although for clarity and succinctness we will use $(u_i)$ instead of $(i \mapsto u_i)$.

However we are more interested in performing arithmetic with these vectors.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Addition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Vector Addition}

Consider pointwise vector addition $U^T + V^T$:
\begin{align}
	U^T + V^T 
	&= \left( (u_i)^{[\![1, k]\!]} \oplus (u'_{i-k})^{(\!(k,n]\!]} \right) 
		\hjoin[+] 
		\left( (v_i)^{[\![1, \ell]\!]} \oplus (v'_{i-\ell})^{(\!(\ell,n]\!]} \right) \\
	&= \left( (u_i)^{[\![1, k]\!]} \oplus (u'_{i-k})^{(\!(k,\ell]\!]} \oplus (u'_{i-k})^{(\!(\ell,n]\!]} \right) 
		\hjoin[+]
		\left( (v_i)^{[\![1, k]\!]} \oplus (v_i)^{(\!(k, \ell]\!]} \oplus (v'_{i-\ell})^{(\!(\ell,n]\!]} \right) \\
	&= \left( (u_i + v_i)^{[\![1, k]\!]} 
		\hjoin[+] (u'_{i-k} + v_i)^{(\!(k,\ell]\!]} 
		\hjoin[+] (u'_{i-k}+v'_{i-\ell})^{(\!(\ell,n]\!]} \right)
\end{align}

This formulation is not unique.

The choice to partition $[\![1,n]\!]$ into $[\![1,k]\!] \oplus (\!(k,\ell]\!] \oplus (\!(\ell, n]\!]$ was arbitrary.

We can just as easily partition $[\![1,n]\!]$ into $[\![1,\ell]\!] \oplus (\!(\ell, k]\!] \oplus (\!(k, n]\!]$ to get the equivalent expression:

\begin{equation}
	U^T + V^T = \left( (u_i + v_i)^{[\![1, \ell]\!]} 
		\hjoin[+] (u_{i} + v'_{i-\ell})^{(\!(\ell,k]\!]} 
		\hjoin[+] (u'_{i-k}+v'_{i-\ell})^{(\!(k,n]\!]} \right)
\end{equation}

We must be careful while evaluating these expressions to not forget that $(u'_{i-k} + v_i)$ is actually shorthand for the function:
\begin{equation*}
	i \mapsto u'_{i-k} + v_i
\end{equation*}

For example, consider the concrete example where $n=5$, $k=4$ and $\ell = 1$ so that
$U^T = [ u_1, u_2, u_3, u_4, u'_1 ]$ and
$V^T = [ v_1, v'_1, v'_2, v'_3, v'_4 ]$.

We will also only assume that the functions $u_i, u'_i, v_i$ and $v'_i$ are defined only on the intervals in which they appear (e.g. $u_5$ is undefined, as is $v'_1$).

Then the expression in (3.19) becomes:
\begin{equation}
(u_i + v_i)^{[\![1,4]\!]} \hjoin[+] (u'_{i-4} + v_i)^{(\!(4,1]\!]} \hjoin[+] (u'_{i-4} + v'_{i-1})^{(\!(1,5]\!]}
\end{equation}

None of the individual subterms cannot be evaluated directly.

In the first term, $v_i$ is not totally defined over the interval $[\![1,4]\!]$.

In the third term, on the interval $(\!(1,5]\!]$, $u'_{i-4}$ would even evaluated on negative indices.

However, these unevaluable terms also appear in the middle term however the interval $(\!(4,1]\!]$ is a negatively oriented interval and the offending points cancel!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Matrix Addition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Addition}


It is common practice in mathematics to represent matrices symbolically with sub-matrices such as:
\begin{equation}
	A = \left[
		\begin{array}{ccc}
			A_1 & A_2 \\
			A_3 & A_4 \\
		\end{array}
	\right]
\end{equation}
If $A$ is an $n \times n$ matrix then $A_1, A_2, A_3, A_4$ are not entries but $(k \times \ell)$, $(n-k \times \ell)$, $(k \times n - \ell)$ and $(n-k \times n - \ell)$ matrices respectively.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Matrix Multiplication
%
% q = k1, r = q2, s = ell1, t=ell2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Matrix Multiplication}

Matrix multiplication


Assume two matrices which we plan to multiply $A$ which is $n \times m$ and $B$ which is $m \times p$.
Both are block matrices:
\begin{equation}
	A = \left[ \begin{array}{cc} A_{11} & A_{12} \\ A_{21} & A_{22} \end{array} \right]
	\;\;\;\;\; \text{and} \;\;\;\;\;
	B = \left[ \begin{array}{cc} B_{11} & B_{12} \\ B_{21} & B_{22} \end{array} \right]
\end{equation}
Where $A_{11}$ is a $q \times r$ matrix and $B_{11}$ is a $s \times t$ matrix.
Note that $0 \leq r , s \leq m$ but the ordering of $r$ and $s$ is unknown.

The simplest case is for $r=s$.
In  this case, 4 regions will arise.
\begin{equation}
	AB = \left[ \begin{array}{cc} 
		A_{11}B_{11}+A_{12}B_{21} & A_{11}B_{12}+A_{12}B_{22} \\ 
		A_{21}B_{11}+A_{22}B_{21} & A_{21}B_{12}+A_{22}B_{22}
	\end{array} \right]
\end{equation}


For the general case ($k_2 \neq \ell_1$), we still get back a $2 \times 2$ block matrix, which we will denote $C$.
\begin{equation}
	AB = \left[ \begin{array}{cc} C_{11} & C_{12} \\ C_{21} & C_{22} \end{array} \right]
\end{equation}
$C_{11}$ is a $k_1 \times \ell_2$ sub-matrix; the sizes of the other partitions can be derived from this.
The domains of these 12 submatrices can be partitioned using:
\begin{equation*}\begin{array}{cc}
	N_1 = [\![0,k_1)\!) & N_2 = [\![k_1, n)\!) 
\end{array}\end{equation*}
\begin{equation*}\begin{array}{cc}
	P_1 = [\![0, \ell_2)\!) & P_2 = [\![ \ell_2, p)\!)
\end{array}\end{equation*}
\begin{equation*}\begin{array}{ccc}
	M_1 = [\![0,k_2)\!) & M_2 = [\![ k_2, \ell_1)\!) & M_3 = [\![ \ell_1, m)\!)
\end{array}\end{equation*}
So we can rewrite $A$ and $B$ as:
\begin{equation}
	A = 	A_{11}^{N_1 \times M_1} \oplus A_{12}^{N_1 \times (M_2 \oplus M_3)} \oplus 
			A_{21}^{N_2 \times M_1} \oplus A_{22}^{N_2 \times (M_2 \oplus M_3)}
\end{equation}
\begin{equation}
	B = 		B_{11}^{(M_1 \oplus M_2) \times P_1} \oplus B_{12}^{(M_1 \oplus M_2) \times P_2} \oplus 
			B_{21}^{M_3 \times P_1} \oplus B_{12}^{M_3 \times P_2}
\end{equation}

Here $\oplus$ is \emph{not} the direct sum of matrices, but hybrid function pointwise sum. 
$\times$ is the Cartesian product of intervals. 

For $i,j \in \{ 1,2 \}$ we can write blocks in $C$ as:
\begin{equation}
	C_{i,j} 	= A_{i,1}^{N_i \times M_1} B_{1,j}^{M_1 \times P_j} 
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3} B_{2,j}^{M_3 \times P_j}
\end{equation}

If $k_2 = \ell_1$ then $M_2 = \emptyset$.
We can think of multiplying a $n \times 0$ matrix by a $0 \times p$ matrix as giving a $n \times p$ matrix which is the sum
over an empty set, 0 everywhere.

If $k_2 < \ell_1$ then this is like treating $A$ and $B$ instead as $2 \times 3$ and $3 \times 2$ block matrices.
$M_1$, $M_2$ and $M_3$ are all positive intervals and form conformable blocks.

If $k_2 > \ell_1$ then $M_2$ is a negative interval which doesn't have a good interpretation.
To simplify $C_i,j$, we can use the partition $ \{ M_1 \oplus M_2, \ominus M_2, M_2 \oplus M_2 \}$ which contains only
positive intervals.


First we rewrite the hybrid functions in the first and third terms:
\begin{align}
	C_{i,j} 	= &\left( A_{i,1}^{N_i \times M_1 \oplus M_2} \oplus A_{i,1}^{N_i \times \ominus M_2} \right)
				\left( B_{1,j}^{M_1 \oplus M_2 \times P_j} \oplus B_{1,j}^{\ominus M_2 \times P_j} \right)
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j} \notag\\
			&+ \left( A_{i,2}^{N_i \times \ominus M_2} \oplus A_{i,2}^{N_i \times M_3 \oplus M_2} \right)
			 \left( B_{2,j}^{\ominus M_2 \times P_j} \oplus B_{2,j}^{M_3 \oplus M_2 \times P_j} \right)
\end{align}

Which are now the product of conformable $1 \times 2$ and $2 \times 1$ block matrices.
This gives us a $1 \times 1$ block matrix :
\begin{align}
	C_{i,j} 	= & A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j}
			+ A_{i,1}^{N_i \times \ominus M_2} B_{1,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_2} B_{1,j}^{M_2 \times P_j} \notag\\
			&+ A_{i,2}^{N_i \times \ominus M_2} B_{2,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j}
\end{align}

Then we merge the middle three terms:
\begin{align}
	C_{i,j} 	=& \left( A_{i,1}^{N_i \times \ominus M_2} 
				\oplus  A_{i,2}^{N_i \times M_2} 
				\oplus A_{i,2}^{N_i \times \ominus M_2} \right)
			\left( B_{1,j}^{\ominus M_2 \times P_j} 
				\oplus B_{1,j}^{M_2 \times P_j} 
				\oplus B_{2,j}^{\ominus M_2 \times P_j} \right)\notag\\
			&+ A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j} 
\end{align}


$A_{i,2}$ occurs twice with opposite sign on the same region and so cancels itself. As does $B_{1,j}$:
\begin{align}
	C_{i,j} 	=& \left( A_{i,1}^{N_i \times \ominus M_2} \oplus  A_{i,2}^{N_i \times \emptyset}  \right)
			\left(  B_{2,j}^{\ominus M_2 \times P_j} \oplus B_{1,j}^{\emptyset \times P_j} \right)\notag\\
			&+ A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j} 
\end{align}
And functions over empty domains can be removed:

\begin{align}
			= & A_{i,1}^{N_i \times M_1 \oplus M_2} B_{1,j}^{M_1 \oplus M_2 \times P_j} 
			+ A_{i,1}^{N_i \times \ominus M_2} B_{2,j}^{\ominus M_2 \times P_j}
			+ A_{i,2}^{N_i \times M_3 \oplus M_2} B_{2,j}^{M_3 \oplus M_2 \times P_j}
\end{align}


Formally this relies on the following identities:

\begin{equation}
	M^B \oplus M^C = M^{B\oplus C}
\end{equation}
(we can split up a matrix into blocks)

\begin{equation}
	M_1^A \oplus M_2^\emptyset = M_1^A = M_2^\emptyset \oplus M_1^A
\end{equation}
(empty blocks can be ignored)

\begin{equation}
	M_1^{A \times B} M_2^{B \times D} + M_3^{A \times C} M_4^{C \times D} 
	= \left(M_1^{A\times B} \oplus M_3^{A \times C} \right) \left( M_2^{B \times D} \oplus M_4^{C \times D} \right)
\end{equation}





















